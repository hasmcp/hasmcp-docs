---
title: How can I view Tool Call Analytics to see my most frequently used tools?
description: Discovering usage trends and optimizing high-traffic endpoints.
---

# Viewing Tool Call Analytics

Understanding which infrastructure endpoints your AI Agents rely on most heavily is critical for prioritizing internal engineering resources. HasMCP provides a dedicated **Analytics Dashboard** exclusively for tracking tool consumption metrics.

![HasMCP Server Analytics Dashboard](/images/kb/server_analytics_tab_1280.png)

### Analyzing Usage Trends
Navigate to the "Analytics" tab in your Workspace. Here, you can segment tool execution metrics by default timeframes (24 Hours, 7 Days, 30 Days) or query any custom time window you desire.

1. **Execution Volume**: The dashboard ranks Provider Tools by sheer execution count. This allows you to identify if your LLMs are constantly polling Jira, but rarely touching Salesforce.
2. **Success/Error Rates**: Each tool displays its execution success percentage. If a specific internal database tool drops to an 85% success rate, it typically indicates network timeouts or faulty authentication downstream that requires immediate engineering intervention.
3. **Latency Profiling**: Analytics natively track the round-trip execution time (p50 latency metrics) for specific tools. If a semantic vector database tool takes 12 seconds to return context, developers can spot the bottleneck and optimize the underlying index.
4. **Token & Payload Savings**: The analytics engine inherently calculates the total volume of data eliminated via Context Optimization, displaying precise Payload Savings (e.g., MBs blocked) and Token Savings for each targeted tool.

### Cost Attribution
By identifying the most frequently utilized endpoints, management can accurately allocate API costs to specific Provider targets. For example, if a specific Serper API tool consumes 80% of daily executions, engineering can decide to aggressively prune those payloads natively using **JMESPath** or **Goja (JS) Interceptors**. This context payload optimization happens *before* the data reaches the LLM. HasMCP automatically tracks the byte difference before and after interception, counting this reduction as compounding **Payload Savings** and **Token Savings** over time, directly driving down your corporate SaaS LLM costs.
